<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Adversarial Attacks on Facial Recognition | Daniel Pedraza</title>
    <meta name="description" content="Research on vulnerabilities in facial recognition systems at Berkman Klein Center">
    <link rel="stylesheet" href="../base-styles.css">
    <link rel="stylesheet" href="../project.css">
</head>
<body>
    <!-- Navigation -->
    <nav>
        <div class="nav-content">
            <a href="../index.html" class="nav-logo">Daniel Pedraza</a>
            <div class="nav-links">
                <a href="../index.html">Home</a>
                <a href="../about.html">About</a>
                <a href="../portfolio.html" class="active">Portfolio</a>
            </div>
        </div>
    </nav>

    <!-- Main Content -->
    <main>
        <a href="../portfolio.html" class="back-link">Back to Portfolio</a>

        <div class="project-header">
            <h1>Adversarial Attacks on Facial Recognition Systems</h1>
            <span class="organization">Berkman Klein Center for Internet & Society, Harvard University</span>
            
            <div class="project-meta">
                <div class="meta-item">
                    <span class="meta-label">Role</span>
                    <span class="meta-value">Research Fellow</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Timeline</span>
                    <span class="meta-value">2017</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Focus Area</span>
                    <span class="meta-value">AI Ethics, Privacy, Computer Vision</span>
                </div>
            </div>
        </div>

        <article class="project-content">
            <div class="key-takeaway">
                <h3>Key Takeaway</h3>
                <p>Technical vulnerabilities in AI systems have real-world consequences. Building systems that respect privacy requires understanding how they can fail—and who they fail for.</p>
            </div>

            <h2>Problem Statement</h2>
            <p>Facial recognition systems were being rapidly deployed across public and private sectors without adequate understanding of their vulnerabilities and potential for misuse. These systems disproportionately impacted marginalized communities through higher error rates and enabled surveillance capabilities that threatened individual privacy and civil liberties.</p>

            <p>The technical research community needed comprehensive analysis of these vulnerabilities, particularly around adversarial attacks that could expose systemic weaknesses. Policy makers required evidence-based research to inform regulation, while the public needed to understand the risks these technologies posed to their fundamental rights.</p>

            <h2>Technical Approach</h2>
            <p>As part of the Assembly fellowship at Berkman Klein Center and MIT Media Lab, our team developed methodologies to test the robustness of commercial facial recognition systems through adversarial attacks. We focused on understanding failure modes across different demographic groups and documenting disparate impacts.</p>

            <p>The research examined:</p>
            <ul>
                <li>Adversarial perturbations that could fool facial recognition algorithms</li>
                <li>Differential vulnerability across skin tones and facial features</li>
                <li>Real-world applicability of adversarial techniques</li>
                <li>Systemic biases embedded in training datasets and model architectures</li>
            </ul>

            <p>We built the <strong>equalAIs</strong> project—a tool that explored algorithmic bias and empowered individuals to understand and potentially subvert problematic computer vision systems. The work bridged technical computer science research with policy implications, translating complex machine learning concepts into actionable insights for non-technical stakeholders.</p>

            <h2>Impact</h2>
            <p>The research contributed to broader conversations about facial recognition regulation and highlighted concerns around disparate impact. Our findings informed policy discussions at both state and federal levels regarding facial recognition deployment in law enforcement and public spaces.</p>

            <p>The work demonstrated that:</p>
            <ul>
                <li>Facial recognition systems have measurable, exploitable vulnerabilities</li>
                <li>These vulnerabilities manifest differently across demographic groups</li>
                <li>Technical testing reveals systemic biases that policy must address</li>
                <li>Individual understanding of these systems can drive accountability</li>
            </ul>

            <h2>What I Learned</h2>
            <p>This project crystallized several insights that have shaped my approach to responsible AI:</p>

            <p><strong>Privacy isn't a feature—it's an architecture decision.</strong> Systems claiming to protect privacy must be designed from the ground up with privacy guarantees, not bolted on afterward. Every architectural choice either strengthens or weakens privacy protections.</p>

            <p><strong>Bias isn't just in the data—it's in the entire pipeline.</strong> From problem formulation to dataset construction to model architecture to deployment context, bias can enter at every stage. Addressing algorithmic fairness requires examining the full sociotechnical system, not just tweaking algorithms.</p>

            <p><strong>Technical research has policy implications.</strong> Working at the intersection of technology and law taught me to translate between technical and policy languages. Research findings must be communicated in ways that enable informed regulation without oversimplifying technical nuance.</p>

            <p><strong>Who a system fails for matters as much as how it fails.</strong> Aggregate accuracy metrics mask disparate impacts. Systems that work well "on average" may systematically harm specific communities. Responsible AI development requires disaggregated analysis and centering the experiences of those most impacted.</p>

            <h2>Broader Context</h2>
            <p>This work was part of a critical moment in AI ethics discourse. It preceded major legislative actions around facial recognition, including moratoria in several cities and ongoing debates about federal regulation. The research contributed to growing recognition that AI systems require proactive governance, not just reactive fixes after harm occurs.</p>

            <p>The lessons from this project directly influenced my subsequent work on privacy-preserving AI deployment at Palantir and informed my approach to building ethical technology systems throughout my career.</p>
        </article>

        <nav class="project-nav">
            <a href="../portfolio.html" class="project-nav-link prev">
                <span class="nav-label">Back to</span>
                <span class="nav-title">Portfolio</span>
            </a>
            <a href="privacy-preserving-llm.html" class="project-nav-link next">
                <span class="nav-label">Next Project</span>
                <span class="nav-title">Privacy-Preserving LLM Framework</span>
            </a>
        </nav>
    </main>

    <!-- Footer -->
    <footer>Daniel Pedraza © 2025</footer>
</body>
</html>
